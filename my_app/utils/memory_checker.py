from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import BaseMessage
from typing import List, Dict, Any

class MemoryCheckerCallback(BaseCallbackHandler):
    """Dumps the final prompt's components, including chat_history."""
    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> None:
        """Run when LLM starts running. Prompts list contains the final prompt."""
        print("\n--- ğŸ›‘ START: FULL LLM PROMPT (w/ Memory) ğŸ›‘ ---")

        # à¸ªà¸³à¸«à¸£à¸±à¸š Chat Model (Gemini), Prompt à¸¡à¸±à¸à¸ˆà¸°à¸–à¸¹à¸à¸ªà¹ˆà¸‡à¹€à¸›à¹‡à¸™ strings à¹ƒà¸™ List
        full_prompt = prompts[0] 
        print(full_prompt)

        print("--- ğŸ›‘ END: FULL LLM PROMPT ğŸ›‘ ---\n")